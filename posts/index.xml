<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Coder</title><link>https://blog.mkdef.com/posts/</link><description>Recent content in Posts on Coder</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 31 Mar 2022 11:49:28 +0800</lastBuildDate><atom:link href="https://blog.mkdef.com/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Wordle优化</title><link>https://blog.mkdef.com/posts/wordle%E4%BC%98%E5%8C%96/</link><pubDate>Thu, 31 Mar 2022 11:49:28 +0800</pubDate><guid>https://blog.mkdef.com/posts/wordle%E4%BC%98%E5%8C%96/</guid><description>结果 目前的代码能通过可以被认为是作弊的方法得到hard模式下的最优解： 初始词 salet, 一共猜测8116次，最多的单词猜7次。
如果需要证明这是最优的，需要遍历所有的单词，枚举所有的可能步骤。这需要大量的时间。不过能力有限，我打算就到此为止了，本文会简单的记录我所发现和使用的一些优化方法。
起点 前文已经给出了目标解的形式，一棵决策树，节点是猜测的单词，状态是转移条件。同时，这棵树是具有最优子结构的。
best[guess] = min(sum(best[next_guess], pattern for next_guess)) for next_guess in all available guesses 从形式上来说，具备树形dp的一切条件。不过这并没有太多改善，因为每一步的状态转移可能性仍然有12000种，而我们也看到最优解最深需要猜7次。这个复杂度是不能接受的。
用整数表示状态 其实这个优化我在很晚才做，虽然这个优化是显然的，不管时间还是空间上，都会得到很大的改善。
但是它会导致状态丧失直观性，我几乎没有办法用肉眼检查计算是否正确。不过多亏了Rust严格的编译期检查，我在后期还算简单地完成了这个修改。
pub fn check(target: &amp;amp;str, guess: &amp;amp;str) -&amp;gt; u8 { let mut freq = BTreeMap::&amp;lt;char, usize&amp;gt;::new(); for (guess_c, target_c) in guess.chars().zip(target.chars()) { if guess_c != target_c { let counter = freq.entry(target_c).or_insert(0); *counter += 1; } } let mut pattern: u8 = 0; let mut base: u8 = 1; for (guess_c, target_c) in guess.</description></item><item><title>Wordle</title><link>https://blog.mkdef.com/posts/wordle/</link><pubDate>Wed, 16 Mar 2022 14:48:07 +0800</pubDate><guid>https://blog.mkdef.com/posts/wordle/</guid><description>起因 在B站上看了3b1b的科普视频，介绍了wordle游戏中的一些信息论原理。在视频里，作者实现了一个solver，并且直观地介绍了它的原理。不过在视频的结尾提到了通过多探索一步，提升了solver的效果。当时看到这里我就在想，难道这个方法并不是最优解？很快就能发现，视频中的每一步guess，都是基于当前的最优值，所以这只是一个简单的贪心算法，很可能不是最优的。当然，视频的主要目的还是直观的介绍信息熵等概念，解法是不是最优并不重要。
后来，我在评论中看到了一个Leader Board, 里面给出了最优解，并且提供了一些解法自己的介绍链接. 然后通过这个名单，我才发现这个Wordle这个游戏还有Hard模式，然后Hard模式的平均猜测次数‘理所当然地’比普通模式的多。然而，后面发现，这里其实提示了一个非常大的坑，这个坑会导致普通模式错过最优解，具体关于这个坑的介绍在本文的最后面。
游戏介绍 Wordle 的游戏规则很简单，玩家每次猜测一个包含5个字母的单词，然后游戏会反馈一个结果，这个结果也包含5位，每个位置有三种颜色，绿色表示该字母正确且位置正确，黄色表示字母正确但是位置不对，灰色表示答案不包含这个字母，最后要求玩家在5次内猜到出这个单词。
思考了一下这个玩法，很容易联想到以前玩过的小游戏猜数字，每次猜测一个四位数字，游戏返回结果XAXB，A代表有几个位置数字和位置都正确，B代表猜对了数字，但是位置不对。
比较这两个游戏，有三个地方是明显的区别：
结果的pattern不一样 猜测的字符wordle可以重复，猜数字不能 wordle要求猜测的5个字符是一个valid的单词，而猜数字没有限制 但是这些区别都不本质，我们可以很容易地把这两个游戏抽象成一个游戏。
首先，定义一个游戏猜测的全集，对于wordle来说，是5个字母的单词表，对于猜数字来说，是所有的4位数字。 玩家给出一个猜测，游戏根据规则的不同会反馈不同的pattern，事实上这可以看做将当前所有可能的结果，根据pattern过滤出符合反馈的，缩小下一轮的猜测范围。 最后不断重复上一个步骤，直到pattern满足胜利条件。 需要注意的是，按照这种模式，给出新的猜测仅与当前可能的结果有关，与之前的猜测和反馈都没有关系。 所以基于这种交互，我们可以定义两个类 Checker 和 Solver :
Checker 相当于游戏的裁判，每一轮的结果以及游戏是否结束都是Checker说了算. Solver 相当于玩家，每一轮根据当前可能的猜测范围提供一个guess，然后根据Checker反馈的结果更新猜测范围。
class Checker: # 根据 target 计算当前 guess 的结果 def check(self, target: str, guess: str) -&amp;gt; str: pass def is_success(self, pattern) -&amp;gt; bool: pass class Solver: def guess(self) -&amp;gt; str: pass def update(self, guess: str, pattern: str): pass 评估策略 想要比较解法的好坏，首先需要合理地评价一个解法。这个游戏通常有两个维度来评价：</description></item><item><title>迁移到Hugo</title><link>https://blog.mkdef.com/posts/%E8%BF%81%E7%A7%BB%E5%88%B0hugo/</link><pubDate>Tue, 25 Jan 2022 23:25:08 +0800</pubDate><guid>https://blog.mkdef.com/posts/%E8%BF%81%E7%A7%BB%E5%88%B0hugo/</guid><description>迁移blog 今天抽时间把blog从Hexo迁移到了Hugo。中间已经很多年没有更新过，想着既然想要重新开始，就干脆迁移到一个新的环境中来。
想要迁移的原因有很多：
首先是真的很不习惯npm和nodejs，之前接触得很少，加上没想到Hexo即使是官方流程走下来，都会有各种报错，没能说服自己无视，又无力折腾。 二来是在折腾LeetCode Notebook in Rust 时，发现Hexo没有可以用的主题，折腾了很多要么丑，要么就是自己魔改了流程。 承接上条，既然Halfrost用的Hugo+hugobook，那当然最简单的思路就是跟进了。试用下来非常丝滑，于是干脆把blog一并迁移，并且更新一篇文章。 安装Hugo Hugo是通过brew安装的，这一点已经观感上已经比Hexo强很多了。即使换成Python，需要我通过Pip安装的tool我也觉得很膈应。
brew install hugo 安装主题 git submodule add https://github.com/luizdepra/hugo-coder.git themes/hugo-coder 拷贝旧文档 需要用到latex的地方加上
math: true Github action 这一步偷懒cat其他项目的配置，多复制了一个%在文件末尾，导致github action找不到 public%目录，但是他又不报错，导致浪费将近一个小时…
name: github pages on: push: branches: - main # Set a branch to deploy pull_request: jobs: deploy: runs-on: ubuntu-20.</description></item><item><title>odes前端改造</title><link>https://blog.mkdef.com/posts/odes%E5%89%8D%E7%AB%AF%E6%94%B9%E9%80%A0/</link><pubDate>Sun, 07 May 2017 22:20:52 +0800</pubDate><guid>https://blog.mkdef.com/posts/odes%E5%89%8D%E7%AB%AF%E6%94%B9%E9%80%A0/</guid><description>Bulma Bulma是一个轻量级的前端框架，仅仅包含CSS文件，而且有很多特别酷炫的主题。第一次看到之后就想用它来写一个前端页面，正好Odes的页面功能比较简单，而且之前实现的时候有很多地方都没有做好，正好借此机会做一个小的改造。
替换掉Bootstrap 因为之前用bootstrap其实也只是为了用它的样式而已，主要功能还是得通过jQuery完成，于是用Bulma之后，就可以毫不犹豫地干掉Bootstrap了。
直接在页面内插入css，然后将bootstrap的样式替换成bulma的样式就可以了。
Bootstrap是包含部分icon的，odes里也有用到，于是还需要添加font-awesome的css才行。
添加面包屑导航 之前的api里其实已经写好了类别的请求、更新的代码，不过因为样式一直调不好的原因，没有显示在界面上。
Bulma最近才刚刚把这个加入到feature list里, 所以只能自己实现。
实现很简单，抄自, html代码如下
&amp;lt;ul class=&amp;#34;breadcrumb&amp;#34;&amp;gt; &amp;lt;li&amp;gt;&amp;lt;/li&amp;gt; &amp;lt;/ul&amp;gt; CSS代码
ul.breadcrumb { outline-style: none; display: block; padding: 0.75rem; padding-left: 0; background: #fff; } ul.breadcrumb li { display: inline-block; } ul.breadcrumb li.is-active { color: #848484; } ul.breadcrumb&amp;gt;li+li:before { padding: 0 5px; color: #848484; content: &amp;#34;/\00a0&amp;#34;; } ul.</description></item><item><title>基于travis和docker的持续集成</title><link>https://blog.mkdef.com/posts/%E5%9F%BA%E4%BA%8Etravis%E5%92%8Cdocker%E7%9A%84%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/</link><pubDate>Mon, 10 Apr 2017 18:41:27 +0800</pubDate><guid>https://blog.mkdef.com/posts/%E5%9F%BA%E4%BA%8Etravis%E5%92%8Cdocker%E7%9A%84%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/</guid><description>本文主要是记录一下最近在两个小项目odes和kraken中使用的持续集成技术。
所使用的基础组件 代码托管在Github上，使用github集成的Travis CI自动触发CI流程。在CI中自动build新的image上传到Docker Hub。然后通过sshpass远程登录server触发部署脚本。部署脚本pull新的image然后部署。
Dockerfile 由于项目都是基于python的，所以dockerfile比较简单：
FROMubuntu:latestMAINTAINERShaobo Liu &amp;lt;shaobo@mkdef.com&amp;gt;LABEL Description=&amp;#34;This image is used to flask-kraken&amp;#34;RUN apt-get update -yRUN apt-get install -y python3-pip python3-dev build-essentialCOPY src /appCOPY requirements.txt /appWORKDIR/appRUN pip3 install -r requirements.txtENTRYPOINT [&amp;#34;python3&amp;#34;]CMD [&amp;#34;app.py&amp;#34;]分解一下：
FROMubuntu:latestMAINTAINERShaobo Liu &amp;lt;shaobo@mkdef.com&amp;gt;LABEL Description=&amp;#34;This image is used to flask-kraken&amp;#34;首先申明使用的基础镜像，然后写上大名表示我是维护这个镜像的作者和这个镜像的用途。
RUN apt-get update -yRUN apt-get install -y python3-pip python3-dev build-essential安装python3，如果有其他的工具或者lib，也要写在这里。
COPY src /appCOPY requirements.txt /appWORKDIR/appRUN pip3 install -r requirements.txt复制源代码到docker里，然后切换工作目录，安装三方依赖。 有时候这里需要安装一些系统级的依赖，比如lxml或者psycopg2之类的，就需要加到前面apt-get install里去。</description></item><item><title>在Gitlab-CI中设置kubectl实现自动部署</title><link>https://blog.mkdef.com/posts/%E5%9C%A8gitlab-ci%E4%B8%AD%E8%AE%BE%E7%BD%AEkubectl%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2/</link><pubDate>Sat, 08 Apr 2017 15:29:39 +0800</pubDate><guid>https://blog.mkdef.com/posts/%E5%9C%A8gitlab-ci%E4%B8%AD%E8%AE%BE%E7%BD%AEkubectl%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2/</guid><description>参考文档 Kubernetes权威指南 How to Add Users to Kubernetes (kubectl)? Kubernetes Documentation 实现思路 Kubernetes集群主要由Master节点控制，Master节点又主要由其中的API Sever(kube-apiserver)提供Rest接口服务。所以我们大体思路就是在CI中配置好正确的kubectl客户端，使它能够直接连到部署的集群上，从而实现远程部署。
目前CI主要分为test, build, deploy等几个阶段，在build阶段就把新的image打包好并且上传到私有registry了，那么deploy阶段就只需要告诉kubernetes新的image编号，kubernetes就会自动帮我们下载新的镜像并且重新部署。
由于目前deploy阶段采用的runner是shell runner，使用的用户是gitlab-runner。可以理解为使用gitlab-runner用户登录到git.hupofin.tech这台机器上去，然后执行ci阶段里定义的命令。
所以我们需要给gitlab-runner配置一个kubectl客户端，这样一来，所有使用shell runner来执行ci的阶段都可以访问集群了
手动完成首次部署 首次部署应用到kubernetes不是本次的重点，可以参考kubernetes官网的交互式tutorial，很清晰。
Service Account 既然是在集群外远程访问控制集群，那么就需要一定的身份认证机制。这里采用Service Account的方式进行验证。
Service Account包括三个部分： namespace，token和CA证书。
所以我们需要做的就是在集群中创建一个新的sa用于部署，然后将上面三样东西配置在kubectl客户端就可以了。
创建一个新的SA kubectl create sa shaobo 获得secret的名字 kubectl get sa shaobo -o json | jq -r .secrets[].name - shaobo-token-rq201 一个Service Account可以包含多个secret，例如刚刚我们创建一个新的SA时自动生成的secret就是用于访问API的secret。 同时，我们也可以自己定义其他的secret，例如在从私有registry下载镜像时可能会需要一个下载镜像的secret，使用https时需要相关证书的secret等等。</description></item><item><title>建立自己的ngrok服务</title><link>https://blog.mkdef.com/posts/%E5%BB%BA%E7%AB%8B%E8%87%AA%E5%B7%B1%E7%9A%84ngrok%E6%9C%8D%E5%8A%A1/</link><pubDate>Mon, 26 Sep 2016 22:40:09 +0800</pubDate><guid>https://blog.mkdef.com/posts/%E5%BB%BA%E7%AB%8B%E8%87%AA%E5%B7%B1%E7%9A%84ngrok%E6%9C%8D%E5%8A%A1/</guid><description>需求 两个需求：
组里采购了一台服务器，托管在所里的机房里，但是到目前为止还没有分配外网ip，所以需要一个将本地端口暴露在外网的解决方案； 台式机是使用的所里的内网，但是本本用的路由。而且偶尔会需要离开工位，此时如果想用本本连到台式机，也需要将本地端口暴露到外网； 用过的解决方案，都是基于ngrok的：
国内的ngrok.cc，其实挺好用的，虽然速度上慢了一点。但是这次升级了版本之后，配置文件的写法没有公布出来，而且如果总是变的话也不是个长久的办法。加上这次服务器被攻击，直接关闭了三天免费服务器，这一点是无法容忍的。
原生的ngrok.io。配置比ngrok.cc要简单一点，但是服务器在美帝，更加慢了。免费版本随机出来的域名实在太难记了，而且每次都会变。并且不能绑定自己的域名，导致这个服务就今天拿来临时用了一下。
初步方案 自然还是用自己的服务器转发一下比较好。不过有两个问题，第一个是自己的服务器也不稳定，第二个是自己的服务器也在美帝。所以这个方案也只能是临时。但之后所里的服务器会有外网ip了，也就不需要这个服务了。其次，之后也许可以在所里的服务器上搭一个ngrok的server，这样就能以很快的速度登录到我的机器了(也许不能的原因是要绑定域名，而这个似乎很困难）
安装golang sudo apt-get install build-essential sudo apt-get install golang sudo apt-get install mercurial 然而我的ubuntu版本似乎太低，安装好的go是1.0的。所以要自己安装高等级的版本，我这里安装的是1.6的版本。
sudo curl -O https://storage.googleapis.com/golang/go1.6.linux-amd64.tar.gz sudo tar -xvf go1.6.linux-amd64.tar.gz sudo mv go /usr/local 然后编辑profile，将go的路劲添加到系统路径里。然而我并没有成功。于是，我采用了很暴力的解决方案：
sudo ln -s /usr/local/go/bin/go /usr/bin/go 然后可以运行一下，查看是否安装成功:
go version 下载编译ngrok git clone https://github.com/inconshreveable/ngrok.git ngrok cd ngrok 接下来，是生成自认证的证书，这里我直接引用人家的一段解释：
Once it has finished cloning ngrok to your local machine, there is yet another thing you will need to do before you can build and compile your very own ngrok server and client and that is to create your own self-signed SSL certificate and this is required because ngrok provides the secure tunnel via TLS and in order for both the client and server to work you will need to build and compile ngrok using your self signed SSL certificate and these are linked to each other.</description></item><item><title>Git 简明指南</title><link>https://blog.mkdef.com/posts/git-%E7%AE%80%E6%98%8E%E6%8C%87%E5%8D%97/</link><pubDate>Mon, 29 Aug 2016 14:05:57 +0800</pubDate><guid>https://blog.mkdef.com/posts/git-%E7%AE%80%E6%98%8E%E6%8C%87%E5%8D%97/</guid><description>Git &amp;amp; Github 介绍 Git首先是一个版本控制系统，它可以持续的追踪你的文件变化情况，从而通过Git，你可以将编辑过的文件恢复到之前的状态，也可以方便查看编辑前后文件内容的差异。
同时，Git也是一个分布式的版本控制系统，对于用户来说，主要体现在允许多个用户协同开发，同时修改代码库。
Github则是一个可以托管代码的平台，同时提供了Web的管理界面和一系列相关的开发服务和功能，功能稳定速度较快，所以在其上托管的项目非常之多，所以用来作为我们的项目托管平台应该是最好的选择。
下载安装 有各种各样的工具可以实现Git的功能，但是由于我们主要采用Github作为托管平台，首先推荐的就是Github自己的图形界面工具： Github Desktop
命令行下只要安装Git就可以了。
事实上，目前大部分的IDE都自带或者通过插件实现了Git的所有功能，像：IntelliJ的所有IDE(包括Android Studio)，Visual Studio， Sublime Text等等。
具体的使用方法请自行谷歌。其实只要掌握了基本的Git命令，各种工具其实都大同小异。
Git基本概念 首先每个都有一个远端服务器仓库，所有的代码都会最终被提交到这里。开发环境中每个开发者都会有一个本地的代码仓库，由于新的代码通常是被开发者首先添加到本地代码仓库中，然后在提交到远端服务器上，所以本地代码的版本通常都要比远端服务器新。但是，如果有其他开发者提交了新的代码到远端服务器，那么这个时候Git就会告诉你，你本地的代码落后于远端服务器版本了，那么你可以选择是否下载最新的版本。
除了开发环境，一个完整的环节应该还包括测试和生产环境，这两个环境的代码都应该对应于远端服务器中的某一个比较完善的版本，远远落后于服务器上的最新版本。且通常只下载代码，不提交代码，也就是它不会对服务器上的版本做任何的改变。这两个环境与我们的项目都关系不大。
新建或者下载一个本地仓库 如果你已经有了一个本地项目，想要使用Git进行版本控制(哪怕你只是新建了一个空的文件夹，还没有任何代码也可以)，这个时候你只需要使用命令：
git init 就可以创建一个新的Git仓库了。
如果你的项目已经有了一个远程的项目仓库(例如我们的代码库是：git@github.com:saukymo/Parkinson_Health.git)，那么我们只需要复制一份远端代码库到本地就可以了，使用命令：
git clone git@github.com:yubozu/Parkinson_Health.git 此时，默认下载的最新版本的代码，当然你也可以选择下载某一特定版本的代码，这里不多说。
本地仓库 本地仓库由三个部分组成，首先是你的工作目录，里面是你的实际项目文件，例如代码、配置等等，第二个是暂存区(stage)，它像是一个缓存区域，你可以随时将修改过的文件添加到暂存区里来，对于这些文件的最新改动都会被记录下来，你可以随时查看它们和之前的区别(diff)。最后一个就是你的版本树(HEAD)了，你需要将暂存区里的改动提交到这里来(commit)，此时就好像给你的项目照了一个快照，目前所有的状态都被保存到了版本树的最前面了。
需要注意的是，暂存区和版本树都是用户不可见的(通常保存在项目根目录的.git文件夹中)，所以你可以放心的修改你的代码而不会影响到git的功能。 一般工作流程 首先，我们直接在修改本地工作目录的代码即可，开发过程与平时完全一样。当你完成一个阶段的开发(或者开发完一个小的功能，或者到了一定时间需要休息一下)时，这时就需要通过Git来提交你之前做的修改了。
git status 首先你需要查看一下本地文件的状态。
git status 这时你将看到你的项目文件中，有哪些被添加到了暂存区中，有哪些还没有(通常是新增加的文件，因为一般来说，一个文件只要被添加到暂存区中一次，它就会一直在暂存区中，不用每次都添加进来)。
git diff 然后你可以通过
git diff 来查看你对这些文件做了哪些修改，改动都是放在一起显示的，红色的代表以前的状态，绿色代表现在的状态。由于这些改动都是你刚刚进行的，所以应该有比较深的印象，这时你应该简单的浏览一遍，回顾一下你做了哪些改动，有没有遗漏(例如 TODO)或者多余的修改(例如 调试时多余的输出等)。</description></item><item><title>Macos安装Caffe和pyCaffe</title><link>https://blog.mkdef.com/posts/macos%E5%AE%89%E8%A3%85caffe%E5%92%8Cpycaffe/</link><pubDate>Fri, 19 Aug 2016 13:21:35 +0800</pubDate><guid>https://blog.mkdef.com/posts/macos%E5%AE%89%E8%A3%85caffe%E5%92%8Cpycaffe/</guid><description>参考阅读 其实主要是翻译的官方文档，官方文档主要是比较全面，各种情况都要考虑到，所以内容比较分散，而很多细节容易被忽略掉。但是如果具体我自己的安装需求，情况比较单一，所以相比之下更加清晰一点。
Macos依赖安装 源码编译
安装依赖包 官方的代码为：
brew install -vd snappy leveldb gflags glog szip lmdb # need the homebrew science source for OpenCV and hdf5 brew tap homebrew/science brew install hdf5 opencv 依次安装就好，每个包的作用如下：
snappy 用于压缩和解压文件
leveldb &amp;amp; lmdb 将各种原始数据通过Key-Value键值对的方式存储在内存中，方便Caffe的DataLayer获取这些数据。 lmdb是比较新的数据管理库，leveldb是比较老的版本，但是为了兼容，仍然保留了这个库。
gflags 解析命令行参数。
glog 日志记录。
BLAS 我不记得有安装这个，可能之前其他项目有装过，这个是主要的矩阵、向量计算的库。根据文档，应该默认使用的ATLAS库。
HDF5 统一的数据存储格式。</description></item><item><title>vps + shadowsocks + switchOmega科学上网</title><link>https://blog.mkdef.com/posts/vps-shadowsocks-switchomega%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/</link><pubDate>Thu, 18 Aug 2016 14:55:11 +0800</pubDate><guid>https://blog.mkdef.com/posts/vps-shadowsocks-switchomega%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/</guid><description>背景 之前用了一段时间的Lantern，虽然很好用，但是流量消耗得太快，跑了一天就超过了80%。正好早上登上了budgetvm的vps整了下数据库，所以就想顺便再把代理捣鼓一下。
经过 借着Lantern剩下的一点流量，没费多少功夫就搜到了神器shadowsocks，但是很遗憾，似乎作者停止更新了。不过好在wiki还是完整的，开始按照wiki折腾了很久，不得要领。遂请教胡神，胡神很给力的给了给了一套教程Get Through the Firewall in Scientific Method。按照教程捣鼓了一番，还是不得要领。最后在胡神亲自带领下，终于成功翻墙，于是写下这篇笔记记录一下方法。
基本按照这个图来
+-------------------------------------+ | Server A(大陆以外) | | 运行shadowsocks-python服务端 | | 对外提供的SS IP为ip1，端口为port1 | +--------------+----------------------+ | | | +--------------+-------------------------------------------+ | 本机 | | 运行shadowsocks-python客户端 + switchOmega socks5代理 | +----------------------------------------------------------+ 本来为了方便使用，应该是在墙内设置一个Server B，然后在B上运行shadowsocks客户端，同时添加cow的二级代理，转成http代理，然后就方便使用了，但是测试了半天没有成功，改用上面的这种简化的方法。缺点在于本机需要运行客户端才可以开启代理。不过都设置好之后也还算方便。
服务器端(墙外VPS)安装和配置 首先安装shadowsocks，因为绑定1000以下端口和后台运行都是需要sudo权限的，所以需要直接在系统库中安装会比较方便一点。
sudo pip install shadowsocks 安装完之后就可以直接运行ssserver命令了。 首先写服务端的配置文件shadowsocks.json：
{ &amp;#34;server&amp;#34;:&amp;#34;▇▇.▇▇.▇▇.▇▇&amp;#34;, &amp;#34;server_port&amp;#34;:9000, &amp;#34;password&amp;#34;:&amp;#34;FUCKXXX&amp;#34;, &amp;#34;timeout&amp;#34;:300, &amp;#34;method&amp;#34;:&amp;#34;aes-256-cfb&amp;#34; } 然后运行</description></item><item><title>Ivita里使用Mysql的一些小技巧</title><link>https://blog.mkdef.com/posts/ivita%E9%87%8C%E4%BD%BF%E7%94%A8mysql%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E6%8A%80%E5%B7%A7/</link><pubDate>Tue, 02 Aug 2016 11:39:19 +0800</pubDate><guid>https://blog.mkdef.com/posts/ivita%E9%87%8C%E4%BD%BF%E7%94%A8mysql%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E6%8A%80%E5%B7%A7/</guid><description>项目背景 因为旅行计划与放假安排没有完全重叠，于是向老板申请调假，在小伙伴们都放高温假的时候，在实验室干活一周。于是被临时安排到了这个叫做Ivita项目里，做的东西其实就是一个手环app。不过因为是临时一周，之后出现问题不好解决，所以只负责了后端的几个API实现。技术框架原本就已经定好，slimePHP + Medoo，数据库是Mysql。然后由于懒癌发作，加上很多功能Medoo并不能实现，于是用了不少以前没有用过的SQL语句和设置。
Table设置 Create和Update的自动记录时间触发 这个其实在用Postgresql的时候就已经用过了，结果发现Mysql更加简单。
created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP, updated_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, 多主键限制 Ivita里uid和date共同作为主键，所以需要给他们俩一起添加一个唯一性限制。
UNIQUE KEY (uid, date) SQL语句 分组求和并排序 业务需求本来求和周年月的每日数据，然后返回一个排名，不过之后这个功能没有意义了，就没有用到这个功能了。这里贴一个更好的：
SELECT orderNumber, FORMAT(SUM(quantityOrdered * priceEach),2) total FROM orderdetails GROUP BY orderNumber ORDER BY SUM(quantityOrdered * priceEach) DESC; 插入重复则更新 很方便的一个功能，本来要写两条还要写代码判断的现在一句话就能解决了，不过Ivita用的Mysql版本不支持Json，所以不知道如果有的字段是Json的话，是不是依然还是那么方便。在Ivita里我把Json字段单独处理的。</description></item><item><title>网站压力测试工具pyWebTest</title><link>https://blog.mkdef.com/posts/%E7%BD%91%E7%AB%99%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/</link><pubDate>Mon, 18 Jul 2016 23:55:39 +0800</pubDate><guid>https://blog.mkdef.com/posts/%E7%BD%91%E7%AB%99%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/</guid><description>项目介绍 pyWebTest是一个性能和负载测试工具，采用纯python编写，可以并发地对目标网站生成请求，测试响应时间和吞吐量，并生成完整的测试报告。
通过简单地修改测试部分的代码，可以很快的实现对远程API的压力测试，事实上，可以通过编写测试脚本，实现很多复杂的测试计划。
测试报告通过标准输出给出。(文件和网页形式的报告功能没有实现)
使用说明 下载和安装 pyWebTest可以通过Git下载完整的源代码，并且不需要额外的设置就可以直接运行。
$ git clone git@github.com:saukymo/pyWebTest.git $ cd pyWebTest $ python main.py -h 使用方法 通过-h参数可以看到简单的参数介绍，一共有4个可选参数，-a是并发进程数，默认模拟两个用户请求网；，-d是测试时间，超过时间测试中止，设定时间之后的请求会被忽略，默认持续10秒；-i是统计时对数据采用的分段数，默认将整个测试分成10段统计；-r是启动进程的时间，所有的并发将在启动时间内均匀的开始，平稳地增加，启动时间到达后，达到预先设定的并发进程数，默认为0秒，即一开始就达到最大负载进行测试。-u是测试的目标网站，不能省略。
$ python main.py -h usage: main.py [-h] [-a [A]] [-d [D]] [-i [I]] [-r [R]] -u U optional arguments: -h, --help show this help message and exit -a [A] number of agents -d [D] test duration in seconds -i [I] number of time-seriels interval -r [R] rampup in seconds -u U test target url 测试和报告样例 样例测试选择百度作为测试对象，一共产生20个用户模拟请求，测试一共持续30秒，统计分成10段，即3秒统计分析一次。启动时间10秒，即每0.</description></item><item><title>Odes（二）页面设计与实现</title><link>https://blog.mkdef.com/posts/odes%E4%BA%8C%E9%A1%B5%E9%9D%A2%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/</link><pubDate>Mon, 20 Jun 2016 13:46:52 +0800</pubDate><guid>https://blog.mkdef.com/posts/odes%E4%BA%8C%E9%A1%B5%E9%9D%A2%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/</guid><description>整体结构 网站按照之前在Everstring常用的前后端结构，后端只提供api，前端一个html从后端请求数据然后展示，用nginx讲两者的流量区分开来。本来前端需要gulp配置一通流水线工具的，但是折腾了一天也没有弄出个样子来，干脆放弃这些高端玩意，先把东西用最原始最简单的方法做出来，有需求了才有动力换新的方法。
后端实现 由于目前只有一个展示诗文正文的一个页面，所以api非常简单。一开始的版本如下：
@app.route(&amp;#34;/odes-api/&amp;lt;int:id&amp;gt;&amp;#34;) def get_ode_by_id(id): script = &amp;#34;&amp;#34;&amp;#34; WITH s AS (SELECT * FROM odes where id = %s) SELECT row_to_json(s) FROM s; &amp;#34;&amp;#34;&amp;#34; cu.execute(script, (id,)) res = cu.fetchall() if res is None: raise ValueError else: return jsonify(res[0]) 请求一个id的数据，把对应诗文的内容和相关的一些数据全部传输过去。这里错误处理暂时没有实现。
后来为了实现上一篇和下一篇的功能，考虑了一番决定还是通过这一个api把相邻两个诗文的数据一并传输过去，对应的sql语句为：
WITH s AS (SELECT * FROM odes where id in (%s, %s, %s)) SELECT array_to_json(array_agg(row_to_json(s))) FROM s; 这样一来，问题就在于前端如何知道哪个数据是自己的，哪个数据是相邻页面的，于是手工调整顺序，将请求的数据放在中间。于是丑陋的代码实现如下：</description></item><item><title>Odes（一）爬取全文</title><link>https://blog.mkdef.com/posts/odes%E4%B8%80%E7%88%AC%E5%8F%96%E5%85%A8%E6%96%87/</link><pubDate>Thu, 16 Jun 2016 10:02:25 +0800</pubDate><guid>https://blog.mkdef.com/posts/odes%E4%B8%80%E7%88%AC%E5%8F%96%E5%85%A8%E6%96%87/</guid><description>项目想法 因为一直没有一个比较完整的项目，所以想参考变卦做一个小的project，展示一个内容有限但是比较有意思的东西，顺便学习实践一下前端技术，于是就选择了诗经。内容不多，一共也就305首。一共分成3个步骤吧，第一步找个网站抓一个比较完整的全文下来，整理好之后存放到数据库里，第二步写一个前端把内容展示出来，但是这边是前后端配合还是单独一个前端页面还没有想好。第三步是其他功能的加入，比如注音和释义，甚至其他的一些比如统计数据之类的功能。
时间安排上并没有计划，主要最近空闲时间比较多，又不想复习，于是才想做这么个项目。反正先把坑挖在这，什么时候能做完就只能随缘了。
postgres安装和配置 其实这一部并不一定需要，因为内容确实不多，直接做成静态页面效果也不错。主要是为了之后第三部可能会需要比较复杂的功能时提前准备。但是没想到这一部花了比较多的时间，最后也只是能用，并没有设置成一个正常的状态。
postgres安装 按照各类教程中的内容，只需要这一步就可以安装好数据库并且自动开启服务，端口5432。
sudo apt-get install postgresql 但是执行完之后，psql并没有连上数据库，服务器上也看不到postgres的进程。
略去中间大量的搜索和尝试的过程，在/usr/lib/postgresql/9.1/bin/目录里找到常用的命令，于是按照手动启动数据库的方式启动：
/usr/lib/postgresql/9.1/bin/postgres -D ~/data 这里提示我不能用root权限开启服务，恍然大悟，之前没有成功启动的原因可能就是因为我一直使用的root用户(使用的服务器虽然用了很长一段时间了，但是当时仅仅安装了一个wordpress就没有登上去过了。所以一直都是root用户。)但是它为什么没有任何提示呢？！于是得到一个教训，新服务器第一件事就是建立一个新的非root用户。建了一个新的用户saukymo之后，终于成功开启了服务。
之后的过程由于大量试错，现在不能准确回忆起来了，不过应该还是通过这个目录下的initdb和createdb成功建立了一个数据库odes。
在Mac中，可以通过brew安装，安装完成之后就可以使用psql了，之后也能安装psycopg2的python库了。
brew install postgresql 直接使用psql命令进入数据库，然后给用户设置密码：
\password saukymo Postgres允许远程连接 默认情况下，只允许本机连接数据库，如果需要远程连接到数据库，需要设置postgres允许远程连接。设置比较简单，首先修改data目录下的pg_hba.conf文件，加入一行
host all all 0.0.0.0/0 md5 这样就能允许所有的ip通过密码访问数据库了。
然后修改postgresl.conf文件，设置listen_addresses为任意即可，即：
listen_addresses = &amp;#39;*&amp;#39; 然后重启服务，我这里因为安装方式不太正确，命令为：
/usr/lib/postgresql/9.1/bin/pg_ctl -D ~/data restart 此时就能在其他机器上连接上数据库了，完整的psql命令为：
psql -U saukymo -h ▇▇.▇▇.▇▇.▇▇ -d odes -W 按照提示输入之前设置的密码即可。
安装psycopg2 这个库是python用来连接postgres数据库的，通过pip安装即可
pip install psycopg2 需要注意的是，这个库并不能兼容Pypy，如果需要和Pypy一起工作的话，需要安装psycopg2cffi，简单设置之后，就可以兼容了，而且原来的代码不需要变化。</description></item><item><title>Sublime Text3 配置</title><link>https://blog.mkdef.com/posts/sublime-text3-%E9%85%8D%E7%BD%AE/</link><pubDate>Mon, 30 May 2016 15:37:48 +0800</pubDate><guid>https://blog.mkdef.com/posts/sublime-text3-%E9%85%8D%E7%BD%AE/</guid><description>Sublime Text3 介绍 Sublime Text3是一款轻量级、跨平台的、拓展功能丰富的编辑器，使用python编写，自然也是用来写python的不二选择。然而，通过丰富强大的插件配置，ST也可以用于其他各种语言的编写。这里主要介绍我常使用的python和Markdown的配置。
Package Control 要想在ST上使用插件，最方便的方法就是通过Package Control来安装，不过我们首先得手工安装它本身。
安装文档可以参考官方的主页，使用Ctrl+` 调出console，复制以下代码
import urllib.request,os,hashlib; h = &amp;#39;2915d1851351e5ee549c20394736b442&amp;#39; + &amp;#39;8bc59f460fa1548d1514676163dafc88&amp;#39;; pf = &amp;#39;Package Control.sublime-package&amp;#39;; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( &amp;#39;http://packagecontrol.io/&amp;#39; + pf.replace(&amp;#39; &amp;#39;, &amp;#39;%20&amp;#39;)).read(); dh = hashlib.sha256(by).hexdigest(); print(&amp;#39;Error validating download (got %s instead of %s), please try manual install&amp;#39; % (dh, h)) if dh != h else open(os.path.join( ipp, pf), &amp;#39;wb&amp;#39; ).</description></item><item><title>常微分方程-初等积分法</title><link>https://blog.mkdef.com/posts/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B-%E5%88%9D%E7%AD%89%E7%A7%AF%E5%88%86%E6%B3%95/</link><pubDate>Sat, 28 May 2016 19:58:35 +0800</pubDate><guid>https://blog.mkdef.com/posts/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B-%E5%88%9D%E7%AD%89%E7%A7%AF%E5%88%86%E6%B3%95/</guid><description>前言 由于复习系统优化理论，需要解常微分方程，然而数分早就还给陈克应老师了，于是在询问老师之后，推荐我看这本书&amp;laquo;常微分方程教程-丁同仁&amp;raquo;，所以就整理了一下这部分笔记。
由于数学基础实在太差，所以忽略所有的细节条件，计算过程中也可能会有大量不严谨的地方，不纠结这些细节。
恰当方程 定义 考虑如下形式的微分方程：
$$P(x,y) dx + Q(x,y)dy = 0$$
如果存在一个函数$\phi (x, y)$满足：
$$d\phi(x, y) = P(x, y)dx + Q(x,y)dy$$
则称原微分方程为恰当方程
那么就有3个问题：
如何判定一个是不是恰当方程？ 如果是，如何求解？ 如果不是，能否转换？ 接下来的几节就是对这3个问题的回答。
恰当方程的判定 充要条件如下：
$$\frac{\alpha P(x,y)}{\alpha y} = \frac{\alpha Q(x, y)}{\alpha x}$$
恰当方程的求解 通解为：
$$\int^{x}{x{0}}P(x,y)dx + \int^{y}{y{0}}Q(x_{0}, y)dy = C$$
或者
$$\int^{x}{x{0}}P(x,y_{0})dx + \int^{y}{y{0}}Q(x, y)dy = C$$</description></item><item><title>微信TCP行为分析</title><link>https://blog.mkdef.com/posts/%E5%BE%AE%E4%BF%A1tcp%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/</link><pubDate>Thu, 19 May 2016 14:36:48 +0800</pubDate><guid>https://blog.mkdef.com/posts/%E5%BE%AE%E4%BF%A1tcp%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/</guid><description>环境设置 实验环境：
iphone6 微信 WeChat6.3.16 macbook pro 分享wifi，网卡en0 wireshark 抓包，并存为Wireshark/tcpdump/&amp;hellip;- pcap格式 tapo在osx中编译失败，所以采用上传数据到另一台ubuntu服务器中分析 测试功能：
聊天功能： 发送文字， 发送图片， 发送小视频；接收文字，接收图片，接收小视频。 朋友圈：发图文状态，发小视频；接收图文状态，接收小视频。 抓包的结果记录如下：
功能 发送者ip和端口 接收者ip和端口 TCP选项 文件名 发送文字 192.168.2.2.53671 223.167.105.117.443 mss 1460,nop,wscale 5,nop,nop,TS val 459787696 ecr 0,sackOK,eol wechat_send_text 发送图片 192.168.2.2.53673 125.39.132.125.443 mss 1460,nop,wscale 5,nop,nop,TS val 458540380 ecr 0,sackOK,eol wechat_send_picture 发送小视频 192.168.2.2.53680 125.39.171.17.443 mss 1460,nop,wscale 5,nop,nop,TS val 458953493 ecr 0,sackOK,eol wechat_send_video 接收文字 223.</description></item><item><title>Brinson绩效分解</title><link>https://blog.mkdef.com/posts/brinson%E7%BB%A9%E6%95%88%E5%88%86%E8%A7%A3/</link><pubDate>Wed, 11 May 2016 13:33:07 +0800</pubDate><guid>https://blog.mkdef.com/posts/brinson%E7%BB%A9%E6%95%88%E5%88%86%E8%A7%A3/</guid><description>由于工作需要评估基金表现，所以尝试了使用最简单直观的Brinson模型来对基金的收益进行分解。通过Brinson分解，我们能够知道基金的收益中有多少来自于对债股的比例配置，有多少来自于对个股的选择。
首先需要说明的是，Brinson能够分解出的因素有很多，但是由于需求和实际数据的限制，这里只考虑资产配置和个股选择两个因素。其次，由于需要比较宽泛的应用到各种基金且各个历史阶段，所以只能对模型作很多简化处理，如果对单一基金做精细分析，Brinson分解能够取得更好的效果。
单期Brinson模型 其实Brinson模型的原理很简单，我们假设有一个基准线，也就是什么额外付出都没有的情况下，一个基金能够获得的收益。然后现在我们待分析的基金比基准多出了一部分收益，那Brinson模型就是用来告诉我们多出来的这部分收益是什么因素导致的。
进一步的，假设这些收益仅仅来自于两个因素，一个因素是资产配置，也就是多少用来买债券，多少用来买股票。如果一定时间段内，股票涨得好，多配一部分股票自然收益会更高一些。另一个因素是个股选择，同样多的钱买股票，我买的股票比你准，涨得比你多，自然我的收益就更一些。
我们用公式来表达，假设基金的实际收益率为$R_{p}$ ，资产配置为$W_{p, i}$, 每种资产的收益率分别为$r_{p, i}$，那么总的收益率就可以表示为
$$ R_{p} = \sum_{i} W_{p,i}r_{p,i} $$
同样，我们用来的参考的基准收益也可以这样分解，表示为
$$ R_{b} = \sum_{i} W_{b,i}r_{b,i} $$
那Brinson模型就把收益分为了四个部分，一个是基准线上的基准收益，第二个是由于你资产配置能力强造成的收益，第三个是由于选股准确造成的收益，最后一个就是由两个部分共同造成的收益。 用表格表示就是
基准行业权重 $w_{b,i}$ 实际行业权重 $w_{p,i}$ 基准行业收益率$r_{b,i}$ 基准组合B $r_{b}=\sum r_{b,i}w_{b,i}$ 配置组合A $r_{a}=\sum r_{b,i}w_{p,i}$ 实际行业收益率$r_{p,i}$ 选股组合S $r_{s}=\sum r_{p,i} w_{b,i}$ 实际组合P $r_{p}=\sum r_{p,i}w_{p,i}$ 所以，对于每个时期的基金表现，找到相应的基准线，计算出以上四个值就可以了。这里我们选取沪深300指数为基准，基准资产配置则为80%股票和20%债券。
数据采集 我们先来分析一下需要哪些数据，首先需要每一支基金在特定时期的收益率，这个数据来源易天富，这个页面支持净值回查，就可以得到所有基金任意时期的收益率了，采集的方法可以参考另一篇文章。
我们需要知道这支基金的资产配置情况，这个可以在这个页面查到。观察一下可以发现，这些数据来源于每个季度的季报，日期确定一年四次，为了简单起见，这里只取股票和债券的比例，其他部分的收益假设为0。需要注意到，这里的数据可能会有错误，有的时候会用0.46表示，有点时候用46.16表示。这个问题很好处理，大于等于1的统一除以100即可。
我们还需要沪深300指数的历史收益，这个数据可以用sohu的api
http://q.stock.sohu.com/hisHq?code=zs_000300&amp;amp;start=20151231&amp;amp;end=20160331 这个api是从这里抓到的，改变日期就可以得到中间所有交易日的净值，非常方便。</description></item><item><title>Macos进行局域网arp欺骗</title><link>https://blog.mkdef.com/posts/macos%E8%BF%9B%E8%A1%8C%E5%B1%80%E5%9F%9F%E7%BD%91arp%E6%AC%BA%E9%AA%97/</link><pubDate>Tue, 10 May 2016 11:09:02 +0800</pubDate><guid>https://blog.mkdef.com/posts/macos%E8%BF%9B%E8%A1%8C%E5%B1%80%E5%9F%9F%E7%BD%91arp%E6%AC%BA%E9%AA%97/</guid><description>之前知乎看到一篇通过手机app就能直接监听到局域网内的其他用户的数据包，并且自动抓取图片和嗅探密码的答案，觉得这样也能成功实在太夸张了。于是自己实现一下，App开发不来，就在Mac上进行好了（事实上用到的也都是别人写好的程序）
安装工具 首先安装Macport, 它是一个类似于apt-get和brew的一个包管理工具，通过port我们可以很方便的安装需要的各种工具。
如果安装之后发现仍然找不到port命令的，可以
export PATH=/opt/local/bin:/opt/local/sbin:$PATH 将其加入到系统路径里来。
port安装好之后就可以开始安装各种工具了
sudo port install arpspoof sudo port install nmap 其实还有sslstrip和ettercap，不过后面两个更多的用于攻击，我们只需要体验一下中间人攻击就好了，事实上是ettercap安装不成功。
nmap扫描整个局域网段 直接执行命令
sudo nmap -sS 192.168.1.0/24 结果如下：
Starting Nmap 7.12 ( https://nmap.org ) at 2016-05-09 17:50 CST Nmap scan report for 192.168.1.1 Host is up (0.0022s latency). Not shown: 998 filtered ports PORT STATE SERVICE 80/tcp open http 1900/tcp open upnp MAC Address: ▇▇:▇▇:▇▇:▇▇:▇▇:▇▇ (Tp-link Technologies) Nmap scan report for 192.</description></item><item><title>Hexo和Github搭建Blog</title><link>https://blog.mkdef.com/posts/hexo%E5%92%8Cgithub%E6%90%AD%E5%BB%BAblog/</link><pubDate>Sun, 17 Apr 2016 10:19:42 +0800</pubDate><guid>https://blog.mkdef.com/posts/hexo%E5%92%8Cgithub%E6%90%AD%E5%BB%BAblog/</guid><description>Hexo Hexo是一个很简洁好用的静态Blog框架，第一次看到是在始终的博客，当时觉得主题很漂亮，速度也还不错，在页面底下看到了Hexo，于是就去看了一下介绍，结果发现好简单啊。加上去年写了一学期Latex，觉得Markdown肯定只会更简单，于是就按照这个教程也搭了一个，于是这篇日志基本就参(zhao)考(chao)这篇教程写了。
准备工作 教程用的windows，我用的Mac，其实差不多，Mac还简单一些，正好给了我个机会证明这篇日志是原创不全是照抄的。
Homebrew Mac下缺的东西都可以用Homwbrew来装，很方便。打开就是安装命令。
Node.js $ brew install npm hexo 然后就可以安装hexo了
$ npm install hexo-cli -g $ npm install hexo --save Hexo配置 初始化 执行以下命令
# 安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。 $ hexo init &amp;lt;folder&amp;gt; $ cd &amp;lt;folder&amp;gt; $ npm install # 新建完成后，指定文件夹的目录如下 . ├── _config.yml ├── package.json ├── scaffolds ├── scripts ├── source | ├── _drafts | └── _posts └── themes 安装插件 这个我是按照教程来的，但是有几个明显用不到的就可以不装了，比如git之外的几个deployer， 还有好几个generator。</description></item><item><title>pypy加速抓取Sina后复权数据</title><link>https://blog.mkdef.com/posts/pypy%E5%8A%A0%E9%80%9F%E6%8A%93%E5%8F%96sina%E5%90%8E%E5%A4%8D%E6%9D%83%E6%95%B0%E6%8D%AE/</link><pubDate>Thu, 14 Apr 2016 14:07:36 +0800</pubDate><guid>https://blog.mkdef.com/posts/pypy%E5%8A%A0%E9%80%9F%E6%8A%93%E5%8F%96sina%E5%90%8E%E5%A4%8D%E6%9D%83%E6%95%B0%E6%8D%AE/</guid><description>pypy PyPy是一个独立的解析器， 通过即时编译(JIT,Just-in-time)代码避免逐行解释执行来提升运行速度。我们一般使用的Python一般是使用C实现的,所以一般又叫CPython。PyPy采用python实现，速度最快可以达到CPython的10倍左右。
PyPy对纯Python的模块支持的非常好，支持的模块可以在这里看到。但是PyPy对C模块的支持还不是很好，主要是对numpy的支持完成度还不够高，所以常用的一些科学运算库也就都不兼容PyPy了。所以个人感觉PyPy主要是应用在服务器和爬虫上。
multiprocessing.dummy multiprocessing.dummy和multiprocessing是两个执行并行任务的库，其中前者是多线程库，后者是多进程库，但是具有相同的api，所以可以很方便的在多线程和多进程之间切换。
由于GIL的原因，Python的多线程其实是单线程交替执行的，所以对于CPU密集的任务来说，多线程其实并不会有很好的效果。但是对于IO密集型的任务，多线程实现简单轻量也有很好的加速效果，值得一试。
举一个简单的爬取网页的例子：
import urllib2 from multiprocessing.dummy import Pool as ThreadPool urls = [] pool = ThreadPool(4) results = pool.map(urllib2.urlopen, urls) pool.close() pool.join() 这个例子采用了4个线程，通过pool.map()来分发任务，结果依次保存在results中，其中pool.join()是等待所有线程结束之后再执行后续的代码。
tushare TuShare是一个免费、开源的python财经数据接口包。它的数据也是从网上各种数据源抓取整理过来的，并且采用统一的结构，返回pandas的dataframe。并且它整合了通联的数据，所以这个包的数据数量和质量都是很不错的。
我的任务是采集A股市场的复权数据，tushare本来可以轻松完成这个任务，但是速度特别慢， 2800支个股4个线程采集需要将近50分钟的时间，对于一个需要每天运行一次的程序来说时间有点长。通过查看源代码，发现其实它是抓取的sina的复权数据页面，而且是每支股票单独抓取的，所以它一共调了将近3K次api来请求整个html页面，然后从中解析出数据。(其实是6K次，因为新浪那个页面是按季度来查询的，然后当时正好跨越了第一和第二两个季度)
于是自然想试试用PyPy来加速了，加上之前也没有用过PyPy，正好通过这个机会来测试一下PyPy的效果。
PyPy的下载和安装 首先在官网的下载页面下载最新版本的PyPy：
wget https://bitbucket.org/pypy/pypy/downloads/pypy-5.0.1-linux64.tar.bz2 然后解压放到任意位置，并用PyPy的作为virtualenv的解释器。
virtualenv -p /path/to/pypy/bin/pypy env source env/bin/activate 此时运行python就能看到PyPy的信息了：
Python 2.7.10 (bbd45126bc69, Mar 18 2016, 21:35:08) [PyPy 5.0.1 with GCC 4.</description></item></channel></rss>