<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>pypy on Coder</title><link>https://blog.mkdef.com/tags/pypy/</link><description>Recent content in pypy on Coder</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 18 Jul 2016 23:55:39 +0000</lastBuildDate><atom:link href="https://blog.mkdef.com/tags/pypy/index.xml" rel="self" type="application/rss+xml"/><item><title>网站压力测试工具pyWebTest</title><link>https://blog.mkdef.com/posts/%E7%BD%91%E7%AB%99%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/</link><pubDate>Mon, 18 Jul 2016 23:55:39 +0000</pubDate><guid>https://blog.mkdef.com/posts/%E7%BD%91%E7%AB%99%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/</guid><description>项目介绍 pyWebTest是一个性能和负载测试工具，采用纯python编写，可以并发地对目标网站生成请求，测试响应时间和吞吐量，并生成完整的测试报告。
通过简单地修改测试部分的代码，可以很快的实现对远程API的压力测试，事实上，可以通过编写测试脚本，实现很多复杂的测试计划。
测试报告通过标准输出给出。(文件和网页形式的报告功能没有实现)
使用说明 下载和安装 pyWebTest可以通过Git下载完整的源代码，并且不需要额外的设置就可以直接运行。
$ git clone git@github.com:saukymo/pyWebTest.git $ cd pyWebTest $ python main.py -h 使用方法 通过-h参数可以看到简单的参数介绍，一共有4个可选参数，-a是并发进程数，默认模拟两个用户请求网；，-d是测试时间，超过时间测试中止，设定时间之后的请求会被忽略，默认持续10秒；-i是统计时对数据采用的分段数，默认将整个测试分成10段统计；-r是启动进程的时间，所有的并发将在启动时间内均匀的开始，平稳地增加，启动时间到达后，达到预先设定的并发进程数，默认为0秒，即一开始就达到最大负载进行测试。-u是测试的目标网站，不能省略。
$ python main.py -h usage: main.py [-h] [-a [A]] [-d [D]] [-i [I]] [-r [R]] -u U optional arguments: -h, --help show this help message and exit -a [A] number of agents -d [D] test duration in seconds -i [I] number of time-seriels interval -r [R] rampup in seconds -u U test target url 测试和报告样例 样例测试选择百度作为测试对象，一共产生20个用户模拟请求，测试一共持续30秒，统计分成10段，即3秒统计分析一次。启动时间10秒，即每0.</description></item><item><title>pypy加速抓取Sina后复权数据</title><link>https://blog.mkdef.com/posts/pypy%E5%8A%A0%E9%80%9F%E6%8A%93%E5%8F%96sina%E5%90%8E%E5%A4%8D%E6%9D%83%E6%95%B0%E6%8D%AE/</link><pubDate>Thu, 14 Apr 2016 14:07:36 +0000</pubDate><guid>https://blog.mkdef.com/posts/pypy%E5%8A%A0%E9%80%9F%E6%8A%93%E5%8F%96sina%E5%90%8E%E5%A4%8D%E6%9D%83%E6%95%B0%E6%8D%AE/</guid><description>pypy PyPy是一个独立的解析器， 通过即时编译(JIT,Just-in-time)代码避免逐行解释执行来提升运行速度。我们一般使用的Python一般是使用C实现的,所以一般又叫CPython。PyPy采用python实现，速度最快可以达到CPython的10倍左右。
PyPy对纯Python的模块支持的非常好，支持的模块可以在这里看到。但是PyPy对C模块的支持还不是很好，主要是对numpy的支持完成度还不够高，所以常用的一些科学运算库也就都不兼容PyPy了。所以个人感觉PyPy主要是应用在服务器和爬虫上。
multiprocessing.dummy multiprocessing.dummy和multiprocessing是两个执行并行任务的库，其中前者是多线程库，后者是多进程库，但是具有相同的api，所以可以很方便的在多线程和多进程之间切换。
由于GIL的原因，Python的多线程其实是单线程交替执行的，所以对于CPU密集的任务来说，多线程其实并不会有很好的效果。但是对于IO密集型的任务，多线程实现简单轻量也有很好的加速效果，值得一试。
举一个简单的爬取网页的例子：
import urllib2 from multiprocessing.dummy import Pool as ThreadPool urls = [] pool = ThreadPool(4) results = pool.map(urllib2.urlopen, urls) pool.close() pool.join() 这个例子采用了4个线程，通过pool.map()来分发任务，结果依次保存在results中，其中pool.join()是等待所有线程结束之后再执行后续的代码。
tushare TuShare是一个免费、开源的python财经数据接口包。它的数据也是从网上各种数据源抓取整理过来的，并且采用统一的结构，返回pandas的dataframe。并且它整合了通联的数据，所以这个包的数据数量和质量都是很不错的。
我的任务是采集A股市场的复权数据，tushare本来可以轻松完成这个任务，但是速度特别慢， 2800支个股4个线程采集需要将近50分钟的时间，对于一个需要每天运行一次的程序来说时间有点长。通过查看源代码，发现其实它是抓取的sina的复权数据页面，而且是每支股票单独抓取的，所以它一共调了将近3K次api来请求整个html页面，然后从中解析出数据。(其实是6K次，因为新浪那个页面是按季度来查询的，然后当时正好跨越了第一和第二两个季度)
于是自然想试试用PyPy来加速了，加上之前也没有用过PyPy，正好通过这个机会来测试一下PyPy的效果。
PyPy的下载和安装 首先在官网的下载页面下载最新版本的PyPy：
wget https://bitbucket.org/pypy/pypy/downloads/pypy-5.0.1-linux64.tar.bz2 然后解压放到任意位置，并用PyPy的作为virtualenv的解释器。
virtualenv -p /path/to/pypy/bin/pypy env source env/bin/activate 此时运行python就能看到PyPy的信息了：
Python 2.7.10 (bbd45126bc69, Mar 18 2016, 21:35:08) [PyPy 5.0.1 with GCC 4.8.4] on linux2 Type &amp;#34;help&amp;#34;, &amp;#34;copyright&amp;#34;, &amp;#34;credits&amp;#34; or &amp;#34;license&amp;#34; for more information.</description></item></channel></rss>