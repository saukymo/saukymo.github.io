<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Python on Coder</title><link>https://blog.mkdef.com/tags/python/</link><description>Recent content in Python on Coder</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 16 Mar 2022 14:48:07 +0800</lastBuildDate><atom:link href="https://blog.mkdef.com/tags/python/index.xml" rel="self" type="application/rss+xml"/><item><title>Wordle</title><link>https://blog.mkdef.com/posts/wordle/</link><pubDate>Wed, 16 Mar 2022 14:48:07 +0800</pubDate><guid>https://blog.mkdef.com/posts/wordle/</guid><description>起因 在B站上看了3b1b的科普视频，介绍了wordle游戏中的一些信息论原理。在视频里，作者实现了一个solver，并且直观地介绍了它的原理。不过在视频的结尾提到了通过多探索一步，提升了solver的效果。当时看到这里我就在想，难道这个方法并不是最优解？很快就能发现，视频中的每一步guess，都是基于当前的最优值，所以这只是一个简单的贪心算法，很可能不是最优的。当然，视频的主要目的还是直观的介绍信息熵等概念，解法是不是最优并不重要。
后来，我在评论中看到了一个Leader Board, 里面给出了最优解，并且提供了一些解法自己的介绍链接. 然后通过这个名单，我才发现这个Wordle这个游戏还有Hard模式，然后Hard模式的平均猜测次数‘理所当然地’比普通模式的多。然而，后面发现，这里其实提示了一个非常大的坑，这个坑会导致普通模式错过最优解，具体关于这个坑的介绍在本文的最后面。
游戏介绍 Wordle 的游戏规则很简单，玩家每次猜测一个包含5个字母的单词，然后游戏会反馈一个结果，这个结果也包含5位，每个位置有三种颜色，绿色表示该字母正确且位置正确，黄色表示字母正确但是位置不对，灰色表示答案不包含这个字母，最后要求玩家在5次内猜到出这个单词。
思考了一下这个玩法，很容易联想到以前玩过的小游戏猜数字，每次猜测一个四位数字，游戏返回结果XAXB，A代表有几个位置数字和位置都正确，B代表猜对了数字，但是位置不对。
比较这两个游戏，有三个地方是明显的区别：
结果的pattern不一样 猜测的字符wordle可以重复，猜数字不能 wordle要求猜测的5个字符是一个valid的单词，而猜数字没有限制 但是这些区别都不本质，我们可以很容易地把这两个游戏抽象成一个游戏。
首先，定义一个游戏猜测的全集，对于wordle来说，是5个字母的单词表，对于猜数字来说，是所有的4位数字。 玩家给出一个猜测，游戏根据规则的不同会反馈不同的pattern，事实上这可以看做将当前所有可能的结果，根据pattern过滤出符合反馈的，缩小下一轮的猜测范围。 最后不断重复上一个步骤，直到pattern满足胜利条件。 需要注意的是，按照这种模式，给出新的猜测仅与当前可能的结果有关，与之前的猜测和反馈都没有关系。 所以基于这种交互，我们可以定义两个类 Checker 和 Solver :
Checker 相当于游戏的裁判，每一轮的结果以及游戏是否结束都是Checker说了算. Solver 相当于玩家，每一轮根据当前可能的猜测范围提供一个guess，然后根据Checker反馈的结果更新猜测范围。
class Checker: # 根据 target 计算当前 guess 的结果 def check(self, target: str, guess: str) -&amp;gt; str: pass def is_success(self, pattern) -&amp;gt; bool: pass class Solver: def guess(self) -&amp;gt; str: pass def update(self, guess: str, pattern: str): pass 评估策略 想要比较解法的好坏，首先需要合理地评价一个解法。这个游戏通常有两个维度来评价：</description></item><item><title>网站压力测试工具pyWebTest</title><link>https://blog.mkdef.com/posts/%E7%BD%91%E7%AB%99%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/</link><pubDate>Mon, 18 Jul 2016 23:55:39 +0800</pubDate><guid>https://blog.mkdef.com/posts/%E7%BD%91%E7%AB%99%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/</guid><description>项目介绍 pyWebTest是一个性能和负载测试工具，采用纯python编写，可以并发地对目标网站生成请求，测试响应时间和吞吐量，并生成完整的测试报告。
通过简单地修改测试部分的代码，可以很快的实现对远程API的压力测试，事实上，可以通过编写测试脚本，实现很多复杂的测试计划。
测试报告通过标准输出给出。(文件和网页形式的报告功能没有实现)
使用说明 下载和安装 pyWebTest可以通过Git下载完整的源代码，并且不需要额外的设置就可以直接运行。
$ git clone git@github.com:saukymo/pyWebTest.git $ cd pyWebTest $ python main.py -h 使用方法 通过-h参数可以看到简单的参数介绍，一共有4个可选参数，-a是并发进程数，默认模拟两个用户请求网；，-d是测试时间，超过时间测试中止，设定时间之后的请求会被忽略，默认持续10秒；-i是统计时对数据采用的分段数，默认将整个测试分成10段统计；-r是启动进程的时间，所有的并发将在启动时间内均匀的开始，平稳地增加，启动时间到达后，达到预先设定的并发进程数，默认为0秒，即一开始就达到最大负载进行测试。-u是测试的目标网站，不能省略。
$ python main.py -h usage: main.py [-h] [-a [A]] [-d [D]] [-i [I]] [-r [R]] -u U optional arguments: -h, --help show this help message and exit -a [A] number of agents -d [D] test duration in seconds -i [I] number of time-seriels interval -r [R] rampup in seconds -u U test target url 测试和报告样例 样例测试选择百度作为测试对象，一共产生20个用户模拟请求，测试一共持续30秒，统计分成10段，即3秒统计分析一次。启动时间10秒，即每0.</description></item><item><title>Odes（二）页面设计与实现</title><link>https://blog.mkdef.com/posts/odes%E4%BA%8C%E9%A1%B5%E9%9D%A2%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/</link><pubDate>Mon, 20 Jun 2016 13:46:52 +0800</pubDate><guid>https://blog.mkdef.com/posts/odes%E4%BA%8C%E9%A1%B5%E9%9D%A2%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/</guid><description>整体结构 网站按照之前在Everstring常用的前后端结构，后端只提供api，前端一个html从后端请求数据然后展示，用nginx讲两者的流量区分开来。本来前端需要gulp配置一通流水线工具的，但是折腾了一天也没有弄出个样子来，干脆放弃这些高端玩意，先把东西用最原始最简单的方法做出来，有需求了才有动力换新的方法。
后端实现 由于目前只有一个展示诗文正文的一个页面，所以api非常简单。一开始的版本如下：
@app.route(&amp;#34;/odes-api/&amp;lt;int:id&amp;gt;&amp;#34;) def get_ode_by_id(id): script = &amp;#34;&amp;#34;&amp;#34; WITH s AS (SELECT * FROM odes where id = %s) SELECT row_to_json(s) FROM s; &amp;#34;&amp;#34;&amp;#34; cu.execute(script, (id,)) res = cu.fetchall() if res is None: raise ValueError else: return jsonify(res[0]) 请求一个id的数据，把对应诗文的内容和相关的一些数据全部传输过去。这里错误处理暂时没有实现。
后来为了实现上一篇和下一篇的功能，考虑了一番决定还是通过这一个api把相邻两个诗文的数据一并传输过去，对应的sql语句为：
WITH s AS (SELECT * FROM odes where id in (%s, %s, %s)) SELECT array_to_json(array_agg(row_to_json(s))) FROM s; 这样一来，问题就在于前端如何知道哪个数据是自己的，哪个数据是相邻页面的，于是手工调整顺序，将请求的数据放在中间。于是丑陋的代码实现如下：</description></item><item><title>Odes（一）爬取全文</title><link>https://blog.mkdef.com/posts/odes%E4%B8%80%E7%88%AC%E5%8F%96%E5%85%A8%E6%96%87/</link><pubDate>Thu, 16 Jun 2016 10:02:25 +0800</pubDate><guid>https://blog.mkdef.com/posts/odes%E4%B8%80%E7%88%AC%E5%8F%96%E5%85%A8%E6%96%87/</guid><description>项目想法 因为一直没有一个比较完整的项目，所以想参考变卦做一个小的project，展示一个内容有限但是比较有意思的东西，顺便学习实践一下前端技术，于是就选择了诗经。内容不多，一共也就305首。一共分成3个步骤吧，第一步找个网站抓一个比较完整的全文下来，整理好之后存放到数据库里，第二步写一个前端把内容展示出来，但是这边是前后端配合还是单独一个前端页面还没有想好。第三步是其他功能的加入，比如注音和释义，甚至其他的一些比如统计数据之类的功能。
时间安排上并没有计划，主要最近空闲时间比较多，又不想复习，于是才想做这么个项目。反正先把坑挖在这，什么时候能做完就只能随缘了。
postgres安装和配置 其实这一部并不一定需要，因为内容确实不多，直接做成静态页面效果也不错。主要是为了之后第三部可能会需要比较复杂的功能时提前准备。但是没想到这一部花了比较多的时间，最后也只是能用，并没有设置成一个正常的状态。
postgres安装 按照各类教程中的内容，只需要这一步就可以安装好数据库并且自动开启服务，端口5432。
sudo apt-get install postgresql 但是执行完之后，psql并没有连上数据库，服务器上也看不到postgres的进程。
略去中间大量的搜索和尝试的过程，在/usr/lib/postgresql/9.1/bin/目录里找到常用的命令，于是按照手动启动数据库的方式启动：
/usr/lib/postgresql/9.1/bin/postgres -D ~/data 这里提示我不能用root权限开启服务，恍然大悟，之前没有成功启动的原因可能就是因为我一直使用的root用户(使用的服务器虽然用了很长一段时间了，但是当时仅仅安装了一个wordpress就没有登上去过了。所以一直都是root用户。)但是它为什么没有任何提示呢？！于是得到一个教训，新服务器第一件事就是建立一个新的非root用户。建了一个新的用户saukymo之后，终于成功开启了服务。
之后的过程由于大量试错，现在不能准确回忆起来了，不过应该还是通过这个目录下的initdb和createdb成功建立了一个数据库odes。
在Mac中，可以通过brew安装，安装完成之后就可以使用psql了，之后也能安装psycopg2的python库了。
brew install postgresql 直接使用psql命令进入数据库，然后给用户设置密码：
\password saukymo Postgres允许远程连接 默认情况下，只允许本机连接数据库，如果需要远程连接到数据库，需要设置postgres允许远程连接。设置比较简单，首先修改data目录下的pg_hba.conf文件，加入一行
host all all 0.0.0.0/0 md5 这样就能允许所有的ip通过密码访问数据库了。
然后修改postgresl.conf文件，设置listen_addresses为任意即可，即：
listen_addresses = &amp;#39;*&amp;#39; 然后重启服务，我这里因为安装方式不太正确，命令为：
/usr/lib/postgresql/9.1/bin/pg_ctl -D ~/data restart 此时就能在其他机器上连接上数据库了，完整的psql命令为：
psql -U saukymo -h ▇▇.▇▇.▇▇.▇▇ -d odes -W 按照提示输入之前设置的密码即可。
安装psycopg2 这个库是python用来连接postgres数据库的，通过pip安装即可
pip install psycopg2 需要注意的是，这个库并不能兼容Pypy，如果需要和Pypy一起工作的话，需要安装psycopg2cffi，简单设置之后，就可以兼容了，而且原来的代码不需要变化。</description></item><item><title>Sublime Text3 配置</title><link>https://blog.mkdef.com/posts/sublime-text3-%E9%85%8D%E7%BD%AE/</link><pubDate>Mon, 30 May 2016 15:37:48 +0800</pubDate><guid>https://blog.mkdef.com/posts/sublime-text3-%E9%85%8D%E7%BD%AE/</guid><description>Sublime Text3 介绍 Sublime Text3是一款轻量级、跨平台的、拓展功能丰富的编辑器，使用python编写，自然也是用来写python的不二选择。然而，通过丰富强大的插件配置，ST也可以用于其他各种语言的编写。这里主要介绍我常使用的python和Markdown的配置。
Package Control 要想在ST上使用插件，最方便的方法就是通过Package Control来安装，不过我们首先得手工安装它本身。
安装文档可以参考官方的主页，使用Ctrl+` 调出console，复制以下代码
import urllib.request,os,hashlib; h = &amp;#39;2915d1851351e5ee549c20394736b442&amp;#39; + &amp;#39;8bc59f460fa1548d1514676163dafc88&amp;#39;; pf = &amp;#39;Package Control.sublime-package&amp;#39;; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( &amp;#39;http://packagecontrol.io/&amp;#39; + pf.replace(&amp;#39; &amp;#39;, &amp;#39;%20&amp;#39;)).read(); dh = hashlib.sha256(by).hexdigest(); print(&amp;#39;Error validating download (got %s instead of %s), please try manual install&amp;#39; % (dh, h)) if dh != h else open(os.path.join( ipp, pf), &amp;#39;wb&amp;#39; ).</description></item><item><title>微信TCP行为分析</title><link>https://blog.mkdef.com/posts/%E5%BE%AE%E4%BF%A1tcp%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/</link><pubDate>Thu, 19 May 2016 14:36:48 +0800</pubDate><guid>https://blog.mkdef.com/posts/%E5%BE%AE%E4%BF%A1tcp%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/</guid><description>环境设置 实验环境：
iphone6 微信 WeChat6.3.16 macbook pro 分享wifi，网卡en0 wireshark 抓包，并存为Wireshark/tcpdump/&amp;hellip;- pcap格式 tapo在osx中编译失败，所以采用上传数据到另一台ubuntu服务器中分析 测试功能：
聊天功能： 发送文字， 发送图片， 发送小视频；接收文字，接收图片，接收小视频。 朋友圈：发图文状态，发小视频；接收图文状态，接收小视频。 抓包的结果记录如下：
功能 发送者ip和端口 接收者ip和端口 TCP选项 文件名 发送文字 192.168.2.2.53671 223.167.105.117.443 mss 1460,nop,wscale 5,nop,nop,TS val 459787696 ecr 0,sackOK,eol wechat_send_text 发送图片 192.168.2.2.53673 125.39.132.125.443 mss 1460,nop,wscale 5,nop,nop,TS val 458540380 ecr 0,sackOK,eol wechat_send_picture 发送小视频 192.168.2.2.53680 125.39.171.17.443 mss 1460,nop,wscale 5,nop,nop,TS val 458953493 ecr 0,sackOK,eol wechat_send_video 接收文字 223.</description></item><item><title>Brinson绩效分解</title><link>https://blog.mkdef.com/posts/brinson%E7%BB%A9%E6%95%88%E5%88%86%E8%A7%A3/</link><pubDate>Wed, 11 May 2016 13:33:07 +0800</pubDate><guid>https://blog.mkdef.com/posts/brinson%E7%BB%A9%E6%95%88%E5%88%86%E8%A7%A3/</guid><description>由于工作需要评估基金表现，所以尝试了使用最简单直观的Brinson模型来对基金的收益进行分解。通过Brinson分解，我们能够知道基金的收益中有多少来自于对债股的比例配置，有多少来自于对个股的选择。
首先需要说明的是，Brinson能够分解出的因素有很多，但是由于需求和实际数据的限制，这里只考虑资产配置和个股选择两个因素。其次，由于需要比较宽泛的应用到各种基金且各个历史阶段，所以只能对模型作很多简化处理，如果对单一基金做精细分析，Brinson分解能够取得更好的效果。
单期Brinson模型 其实Brinson模型的原理很简单，我们假设有一个基准线，也就是什么额外付出都没有的情况下，一个基金能够获得的收益。然后现在我们待分析的基金比基准多出了一部分收益，那Brinson模型就是用来告诉我们多出来的这部分收益是什么因素导致的。
进一步的，假设这些收益仅仅来自于两个因素，一个因素是资产配置，也就是多少用来买债券，多少用来买股票。如果一定时间段内，股票涨得好，多配一部分股票自然收益会更高一些。另一个因素是个股选择，同样多的钱买股票，我买的股票比你准，涨得比你多，自然我的收益就更一些。
我们用公式来表达，假设基金的实际收益率为$R_{p}$ ，资产配置为$W_{p, i}$, 每种资产的收益率分别为$r_{p, i}$，那么总的收益率就可以表示为
$$ R_{p} = \sum_{i} W_{p,i}r_{p,i} $$
同样，我们用来的参考的基准收益也可以这样分解，表示为
$$ R_{b} = \sum_{i} W_{b,i}r_{b,i} $$
那Brinson模型就把收益分为了四个部分，一个是基准线上的基准收益，第二个是由于你资产配置能力强造成的收益，第三个是由于选股准确造成的收益，最后一个就是由两个部分共同造成的收益。 用表格表示就是
基准行业权重 $w_{b,i}$ 实际行业权重 $w_{p,i}$ 基准行业收益率$r_{b,i}$ 基准组合B $r_{b}=\sum r_{b,i}w_{b,i}$ 配置组合A $r_{a}=\sum r_{b,i}w_{p,i}$ 实际行业收益率$r_{p,i}$ 选股组合S $r_{s}=\sum r_{p,i} w_{b,i}$ 实际组合P $r_{p}=\sum r_{p,i}w_{p,i}$ 所以，对于每个时期的基金表现，找到相应的基准线，计算出以上四个值就可以了。这里我们选取沪深300指数为基准，基准资产配置则为80%股票和20%债券。
数据采集 我们先来分析一下需要哪些数据，首先需要每一支基金在特定时期的收益率，这个数据来源易天富，这个页面支持净值回查，就可以得到所有基金任意时期的收益率了，采集的方法可以参考另一篇文章。
我们需要知道这支基金的资产配置情况，这个可以在这个页面查到。观察一下可以发现，这些数据来源于每个季度的季报，日期确定一年四次，为了简单起见，这里只取股票和债券的比例，其他部分的收益假设为0。需要注意到，这里的数据可能会有错误，有的时候会用0.46表示，有点时候用46.16表示。这个问题很好处理，大于等于1的统一除以100即可。
我们还需要沪深300指数的历史收益，这个数据可以用sohu的api
http://q.stock.sohu.com/hisHq?code=zs_000300&amp;amp;start=20151231&amp;amp;end=20160331 这个api是从这里抓到的，改变日期就可以得到中间所有交易日的净值，非常方便。</description></item><item><title>pypy加速抓取Sina后复权数据</title><link>https://blog.mkdef.com/posts/pypy%E5%8A%A0%E9%80%9F%E6%8A%93%E5%8F%96sina%E5%90%8E%E5%A4%8D%E6%9D%83%E6%95%B0%E6%8D%AE/</link><pubDate>Thu, 14 Apr 2016 14:07:36 +0800</pubDate><guid>https://blog.mkdef.com/posts/pypy%E5%8A%A0%E9%80%9F%E6%8A%93%E5%8F%96sina%E5%90%8E%E5%A4%8D%E6%9D%83%E6%95%B0%E6%8D%AE/</guid><description>pypy PyPy是一个独立的解析器， 通过即时编译(JIT,Just-in-time)代码避免逐行解释执行来提升运行速度。我们一般使用的Python一般是使用C实现的,所以一般又叫CPython。PyPy采用python实现，速度最快可以达到CPython的10倍左右。
PyPy对纯Python的模块支持的非常好，支持的模块可以在这里看到。但是PyPy对C模块的支持还不是很好，主要是对numpy的支持完成度还不够高，所以常用的一些科学运算库也就都不兼容PyPy了。所以个人感觉PyPy主要是应用在服务器和爬虫上。
multiprocessing.dummy multiprocessing.dummy和multiprocessing是两个执行并行任务的库，其中前者是多线程库，后者是多进程库，但是具有相同的api，所以可以很方便的在多线程和多进程之间切换。
由于GIL的原因，Python的多线程其实是单线程交替执行的，所以对于CPU密集的任务来说，多线程其实并不会有很好的效果。但是对于IO密集型的任务，多线程实现简单轻量也有很好的加速效果，值得一试。
举一个简单的爬取网页的例子：
import urllib2 from multiprocessing.dummy import Pool as ThreadPool urls = [] pool = ThreadPool(4) results = pool.map(urllib2.urlopen, urls) pool.close() pool.join() 这个例子采用了4个线程，通过pool.map()来分发任务，结果依次保存在results中，其中pool.join()是等待所有线程结束之后再执行后续的代码。
tushare TuShare是一个免费、开源的python财经数据接口包。它的数据也是从网上各种数据源抓取整理过来的，并且采用统一的结构，返回pandas的dataframe。并且它整合了通联的数据，所以这个包的数据数量和质量都是很不错的。
我的任务是采集A股市场的复权数据，tushare本来可以轻松完成这个任务，但是速度特别慢， 2800支个股4个线程采集需要将近50分钟的时间，对于一个需要每天运行一次的程序来说时间有点长。通过查看源代码，发现其实它是抓取的sina的复权数据页面，而且是每支股票单独抓取的，所以它一共调了将近3K次api来请求整个html页面，然后从中解析出数据。(其实是6K次，因为新浪那个页面是按季度来查询的，然后当时正好跨越了第一和第二两个季度)
于是自然想试试用PyPy来加速了，加上之前也没有用过PyPy，正好通过这个机会来测试一下PyPy的效果。
PyPy的下载和安装 首先在官网的下载页面下载最新版本的PyPy：
wget https://bitbucket.org/pypy/pypy/downloads/pypy-5.0.1-linux64.tar.bz2 然后解压放到任意位置，并用PyPy的作为virtualenv的解释器。
virtualenv -p /path/to/pypy/bin/pypy env source env/bin/activate 此时运行python就能看到PyPy的信息了：
Python 2.7.10 (bbd45126bc69, Mar 18 2016, 21:35:08) [PyPy 5.0.1 with GCC 4.</description></item></channel></rss>